"""
Script de diagnostic pour LiteLLM.

V√©rifie la connectivit√© r√©seau et les probl√®mes potentiels.
"""

import asyncio
import os
import sys

import httpx
from dotenv import load_dotenv

from browser_use.llm.openai.utils import normalize_openai_base_url

load_dotenv()


async def diagnose():
	"""Diagnostic complet de la connexion LiteLLM."""
	url = os.getenv('OPENAI_API_URL', '')
	normalized_url = normalize_openai_base_url(url)
	api_key = os.getenv('OPENAI_API_KEY', '')
	
	print("üîç Diagnostic de connexion LiteLLM\n")
	print(f"URL: {url}")
	if normalized_url and normalized_url != url:
		print(f"URL normalis√©e: {normalized_url}")
	print(f"API Key: {'‚úÖ D√©finie' if api_key else '‚ùå Non d√©finie'}")
	print()
	
	if not url:
		print("‚ùå OPENAI_API_URL non d√©fini")
		return
	
	# Test 1: Connectivit√© r√©seau de base
	print("1Ô∏è‚É£  Test de connectivit√© r√©seau...")
	try:
		async with httpx.AsyncClient(timeout=5.0) as client:
			# Essayer juste de se connecter
			response = await client.get(url, follow_redirects=True)
			print(f"   ‚úÖ Serveur accessible (HTTP {response.status_code})")
	except httpx.TimeoutException:
		print("   ‚ùå Timeout - Le serveur ne r√©pond pas")
		print("   üí° V√©rifiez:")
		print("      - Que vous √™tes sur le bon r√©seau/VPN")
		print("      - Que le serveur LiteLLM est en cours d'ex√©cution")
		print("      - Que l'URL est correcte")
		return
	except Exception as e:
		print(f"   ‚ö†Ô∏è  Erreur: {type(e).__name__}: {str(e)}")
	
	# Test 2: Endpoint /health si disponible
	print("\n2Ô∏è‚É£  Test de l'endpoint /health...")
	try:
		health_url = url.rstrip('/') + '/health'
		async with httpx.AsyncClient(timeout=5.0) as client:
			response = await client.get(health_url)
			print(f"   ‚úÖ Health check OK (HTTP {response.status_code})")
	except Exception as e:
		print(f"   ‚ÑπÔ∏è  /health non disponible ({type(e).__name__})")
	
	# Test 3: Endpoint /v1/models
	print("\n3Ô∏è‚É£  Test de l'endpoint /v1/models...")
	try:
		api_base = normalized_url or (url.rstrip('/') + '/v1')
		models_url = api_base.rstrip('/') + '/models'
		async with httpx.AsyncClient(timeout=10.0) as client:
			headers = {}
			if api_key:
				headers['Authorization'] = f'Bearer {api_key}'
			response = await client.get(models_url, headers=headers)
			if response.status_code == 200:
				print(f"   ‚úÖ Liste des mod√®les accessible")
				try:
					data = response.json()
					models = data.get('data', [])
					print(f"   üìã {len(models)} mod√®le(s) disponible(s)")
					if models:
						print("   Mod√®les:")
						for model in models[:5]:  # Afficher les 5 premiers
							model_id = model.get('id', 'N/A')
							print(f"      - {model_id}")
				except:
					pass
			else:
				print(f"   ‚ö†Ô∏è  HTTP {response.status_code}: {response.text[:100]}")
	except httpx.TimeoutException:
		print("   ‚ùå Timeout - Le serveur ne r√©pond pas aux requ√™tes API")
		print("   üí° Le serveur est accessible mais ne r√©pond pas aux requ√™tes.")
		print("      Cela peut indiquer:")
		print("      - Un probl√®me de configuration du serveur LiteLLM")
		print("      - Un probl√®me de routage r√©seau")
		print("      - Le serveur est surcharg√©")
	except Exception as e:
		print(f"   ‚ùå Erreur: {type(e).__name__}: {str(e)}")
	
	# Test 4: Test d'un appel simple
	print("\n4Ô∏è‚É£  Test d'un appel chat simple...")
	if not api_key:
		print("   ‚ö†Ô∏è  API key manquante, test ignor√©")
	else:
		try:
			api_base = normalized_url or (url.rstrip('/') + '/v1')
			chat_url = api_base.rstrip('/') + '/chat/completions'
			async with httpx.AsyncClient(timeout=30.0) as client:
				headers = {
					'Authorization': f'Bearer {api_key}',
					'Content-Type': 'application/json',
				}
				payload = {
					'model': 'gemini-2.5-flash-lite-preview-09-2025',
					'messages': [{'role': 'user', 'content': 'Test'}],
					'max_tokens': 10,
				}
				response = await client.post(chat_url, json=payload, headers=headers)
				if response.status_code == 200:
					print("   ‚úÖ Appel chat r√©ussi!")
				else:
					print(f"   ‚ö†Ô∏è  HTTP {response.status_code}")
					print(f"   R√©ponse: {response.text[:200]}")
		except httpx.TimeoutException:
			print("   ‚ùå Timeout - Le serveur ne r√©pond pas aux appels chat")
			print("   üí° Le mod√®le peut √™tre tr√®s lent ou le serveur surcharg√©")
		except Exception as e:
			print(f"   ‚ùå Erreur: {type(e).__name__}: {str(e)}")
	
	print("\n" + "="*50)
	print("üí° Recommandations:")
	print("   1. V√©rifiez avec l'√©quipe qui g√®re le serveur LiteLLM")
	print("   2. Testez avec un mod√®le plus rapide (sans 'thinking')")
	print("   3. V√©rifiez les logs du serveur LiteLLM")
	print("   4. Essayez depuis un autre r√©seau pour isoler le probl√®me")


if __name__ == "__main__":
	asyncio.run(diagnose())
