"""
Agent designed to build a lightweight list of startups from directories such as
Product Hunt, BetaList, FutureTools, etc.
"""

from __future__ import annotations

import argparse
import asyncio
import copy
import json
import os
import re
from pathlib import Path
from textwrap import dedent
from urllib.parse import urljoin, urlparse

import httpx
from dotenv import load_dotenv
from pydantic import AnyHttpUrl, BaseModel, Field, ValidationError

from browser_use import Agent, ChatBrowserUse, ChatOpenAI

# Load environment variables immediately so the agent can access API keys.
load_dotenv()
# Use slightly slower timeouts than defaults to let the agent finish scrolling/extractions.
os.environ.setdefault('TIMEOUT_ScreenshotEvent', '25')
os.environ.setdefault('TIMEOUT_BrowserStateRequestEvent', '45')


class StartupListingInput(BaseModel):
	"""User-provided parameters for the startup listing task."""

	url: AnyHttpUrl = Field(..., description='Product Hunt, BetaList, FutureTools, etc. listing URL')
	max_startups: int = Field(
		12,
		ge=1,
		le=40,
		description='Maximum number of startups to capture from the page',
	)
	output_path: Path = Field(
		default=Path('startup_listings.json'),
		description='Destination for the JSON list of startups',
	)


class StartupProfile(BaseModel):
	"""Minimal structured information for each startup entry."""

	name: str = Field(..., description='Startup name exactly as written on the listing')
	listing_url: str | None = Field(
		None,
		description='Direct URL to the startup page/product as exposed by the listing',
	)
	linkedin_url: str | None = Field(
		None,
		description='Public LinkedIn URL shown on the listing (keep None if absent)',
	)
	short_notes: list[str] = Field(
		default_factory=list,
		description='Two or three short bullet points from the listing (value proposition, positioning, tags, etc.)',
	)


class StartupListingReport(BaseModel):
	"""Complete response returned by the agent."""

	source_url: AnyHttpUrl = Field(..., description='URL that was analysed')
	startups: list[StartupProfile] = Field(
		...,
		min_length=1,
		description='Startup entries ordered as they appear on the listing',
	)


def _normalize_linkedin_url(value: str | None) -> str | None:
	"""Return a valid LinkedIn URL or None."""

	if not value:
		return None

	url = value.strip()
	if not url:
		return None

	if not url.lower().startswith(('http://', 'https://')):
		return None

	parsed = urlparse(url)
	if not parsed.netloc:
		return None

	if 'linkedin.com' not in parsed.netloc.lower():
		return None

	return url


def _fallback_report(source_url: str, reason: str) -> StartupListingReport:
	"""Return a minimal report when the agent cannot finish properly."""

	reason = reason.strip() or "Impossible d'obtenir un listing fiable depuis la page."
	from pydantic import AnyHttpUrl
	return StartupListingReport(
		source_url=AnyHttpUrl(source_url),
		startups=[
			StartupProfile(
				name='Informations indisponibles',
				listing_url=source_url,
				linkedin_url=None,
				short_notes=[
					reason,
					'Rapport gÃ©nÃ©rÃ© automatiquement (agent interrompu avant la fin).',
				],
			)
		],
	)


def _normalize_listing_url(url: str | None, base_url: str) -> str | None:
	"""Convert relative URLs to absolute URLs."""
	if not url:
		return None
	
	url = url.strip()
	if not url:
		return None
	
	# If it's already an absolute URL, return as is
	if url.startswith(('http://', 'https://')):
		return url
	
	# If it starts with /, make it relative to the base domain
	if url.startswith('/'):
		parsed_base = urlparse(base_url)
		return f"{parsed_base.scheme}://{parsed_base.netloc}{url}"
	
	# Otherwise, try to resolve relative to base URL
	try:
		return urljoin(base_url, url)
	except Exception:
		return url


def _sanitize_report(report: StartupListingReport) -> StartupListingReport:
	"""Apply basic clean-up rules on top of the structured output."""

	base_url = str(report.source_url)
	for startup in report.startups:
		startup.linkedin_url = _normalize_linkedin_url(startup.linkedin_url)
		startup.listing_url = _normalize_listing_url(startup.listing_url, base_url)
		if startup.short_notes:
			startup.short_notes = [note.strip() for note in startup.short_notes if note.strip()]
	return report


def build_task(task_input: StartupListingInput) -> str:
	"""Create the natural-language instructions fed to the agent."""

	return dedent(
		f"""
		Tu es un analyste chargÃ© de dresser un simple listing de startups Ã  partir de la page {task_input.url}.

		Objectif:
		- Identifie jusqu'Ã  {task_input.max_startups} startups ou produits prÃ©sentÃ©s sur cette page.
		- Pour chaque entrÃ©e, capture:
		  â€¢ `name`: nom affichÃ©.
		  â€¢ `listing_url`: URL exacte du bouton ou lien principal (utilise l'attribut href original, pas du texte).
		  â€¢ `linkedin_url`: URL LinkedIn si visible sur la page (laisse null sinon).
		  â€¢ `short_notes`: 2-3 infos trÃ¨s courtes depuis la page (tagline, cas d'usage, prix, catÃ©gorie, etc.).

		Processus recommandÃ©:
		1. Scrolle la page plusieurs fois pour charger tous les listings (utilise `scroll` avec `down: true` et `pages: 1`).
		2. Utilise `extract` avec `extract_links=true` pour rÃ©cupÃ©rer les donnÃ©es structurÃ©es des startups.
		3. Une fois que tu as collectÃ© toutes les donnÃ©es, utilise l'action `done` avec le champ `data` contenant l'objet `StartupListingReport` complet.

		RÃ¨gles importantes:
		- Reste strictement sur la page fournie; ne fais aucun aller-retour externe, aucune recherche additionnelle.
		- Scrolle l'intÃ©gralitÃ© du listing et n'oublie aucune carte pertinente.
		- Ã€ chaque appel `scroll`, fournis toujours `down` ET `pages` (ex: {{"scroll": {{"down": true, "pages": 1}}}}).
		- Lorsque tu extrais du texte, spÃ©cifie un champ `query`; pour rÃ©cupÃ©rer des URLs, ajoute `extract_links=true`.
		- Les `short_notes` doivent Ãªtre des phrases concises (<= 140 caractÃ¨res) ou des puces factuelles, uniquement depuis le contenu affichÃ©.
		- Si une info manque, laisse le champ vide (chaine vide) ou null, mais ne l'invente pas.
		- IMPORTANT: Pour terminer la tÃ¢che, utilise l'action `done` avec le format suivant:
		  {{"done": {{"success": true, "data": {{"source_url": "{task_input.url}", "startups": [...]}}}}}}
		- Le champ `data` de `done` doit contenir un objet `StartupListingReport` avec `source_url` et `startups` (liste de `StartupProfile`).
		- Chaque `StartupProfile` doit avoir `name`, `listing_url` (ou null), `linkedin_url` (ou null), et `short_notes` (liste de chaÃ®nes).
		- Exemple de format attendu pour `done`:
		  {{"done": {{"success": true, "data": {{
		    "source_url": "{task_input.url}",
		    "startups": [
		      {{
		        "name": "Nom de la startup",
		        "listing_url": "https://www.producthunt.com/products/...",
		        "linkedin_url": null,
		        "short_notes": ["Tagline", "CatÃ©gorie", "Prix"]
		      }}
		    ]
		  }}}}}}
		- Utilise la vision et sois patient si le chargement est lent; rÃ©essaie plutÃ´t que d'abandonner.
		- Ne termine la tÃ¢che qu'aprÃ¨s avoir construit l'objet `StartupListingReport` complet dans le champ `data` de `done`.
		"""
	).strip()


async def run_startup_listing(task_input: StartupListingInput) -> StartupListingReport | None:
	"""Execute the agent and return the structured list of startups."""

	print("ğŸ”§ Configuration du LLM...")
	if os.getenv('BROWSER_USE_API_KEY'):
		llm = ChatBrowserUse()
		print("âœ… Utilisation de ChatBrowserUse")
	else:
		model_name = os.getenv('OPENAI_MODEL', 'gemini-2.5-flash-lite-preview-09-2025')
		# Pour les modÃ¨les Gemini via LiteLLM, utiliser add_schema_to_system_prompt
		# pour Ã©viter les problÃ¨mes de schÃ©ma JSON avec response_format
		is_gemini = 'gemini' in model_name.lower()
		llm = ChatOpenAI(
			model=model_name,
			timeout=httpx.Timeout(180.0, connect=60.0, read=180.0, write=30.0),
			max_retries=3,  # AugmentÃ© pour plus de robustesse avec Gemini
			add_schema_to_system_prompt=is_gemini,  # Ã‰vite les problÃ¨mes de schÃ©ma avec Gemini
			dont_force_structured_output=is_gemini,  # Gemini via LiteLLM a des problÃ¨mes avec response_format
		)
		print(f"âœ… Utilisation de ChatOpenAI avec le modÃ¨le: {model_name}")
		if is_gemini:
			print("   âš ï¸  Mode Gemini dÃ©tectÃ©: utilisation du schÃ©ma dans le prompt systÃ¨me")
			print("   ğŸ’¡ Note: Gemini peut parfois gÃ©nÃ©rer du JSON mal formÃ©, mais les donnÃ©es seront rÃ©cupÃ©rÃ©es depuis les extractions.")

	print("ğŸ¤– CrÃ©ation de l'agent...")
	agent = Agent(
		task=build_task(task_input),
		llm=llm,
		output_model_schema=StartupListingReport,
		use_vision=True,
		vision_detail_level='high',
		step_timeout=300,
		llm_timeout=180,
		max_failures=5,
		directly_open_url=True,
	)
	print("âœ… Agent crÃ©Ã©")

	print("â–¶ï¸  DÃ©marrage de l'exÃ©cution de l'agent...")
	history = await agent.run()
	print("âœ… ExÃ©cution terminÃ©e")
	
	# Check if agent completed successfully
	agent_successful = history.is_successful()
	if not agent_successful and history.has_errors():
		print("âš ï¸  ATTENTION: Il semble y avoir eu un problÃ¨me avec l'agent, mais on va essayer d'extraire les donnÃ©es quand mÃªme.")
	
	# Try to get structured output first
	if history.structured_output:
		return _sanitize_report(history.structured_output)  # type: ignore[arg-type]

	# Try to extract from final result
	final_result = history.final_result()
	if final_result:
		try:
			# Try to parse as JSON
			report = StartupListingReport.model_validate_json(final_result)
			if not agent_successful:
				print("âš ï¸  DonnÃ©es rÃ©cupÃ©rÃ©es depuis le rÃ©sultat final malgrÃ© l'Ã©chec de l'agent.")
			return _sanitize_report(report)
		except ValidationError:
			# Try to extract JSON from markdown code blocks
			json_match = re.search(r'```(?:json)?\s*(\{.*\})\s*```', final_result, re.DOTALL)
			if json_match:
				try:
					report = StartupListingReport.model_validate_json(json_match.group(1))
					if not agent_successful:
						print("âš ï¸  DonnÃ©es rÃ©cupÃ©rÃ©es depuis le rÃ©sultat final (markdown) malgrÃ© l'Ã©chec de l'agent.")
					return _sanitize_report(report)
				except ValidationError:
					pass

	# Try to extract from action results (especially extract actions)
	extracted_contents = history.extracted_content()
	for content in reversed(extracted_contents):  # Start from most recent
		if not content:
			continue
		try:
			# Try to parse as JSON directly
			report = StartupListingReport.model_validate_json(content)
			if not agent_successful:
				print("âš ï¸  DonnÃ©es rÃ©cupÃ©rÃ©es depuis les rÃ©sultats d'extraction malgrÃ© l'Ã©chec de l'agent.")
			return _sanitize_report(report)
		except ValidationError:
			# Try to extract JSON from markdown
			json_match = re.search(r'```(?:json)?\s*(\{.*\})\s*```', content, re.DOTALL)
			if json_match:
				try:
					report = StartupListingReport.model_validate_json(json_match.group(1))
					if not agent_successful:
						print("âš ï¸  DonnÃ©es rÃ©cupÃ©rÃ©es depuis les rÃ©sultats d'extraction (markdown) malgrÃ© l'Ã©chec de l'agent.")
					return _sanitize_report(report)
				except ValidationError:
					pass
			# Try to find JSON object in the content
			json_match = re.search(r'(\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})', content, re.DOTALL)
			if json_match:
				try:
					report = StartupListingReport.model_validate_json(json_match.group(1))
					if not agent_successful:
						print("âš ï¸  DonnÃ©es rÃ©cupÃ©rÃ©es depuis les rÃ©sultats d'extraction (JSON brut) malgrÃ© l'Ã©chec de l'agent.")
					return _sanitize_report(report)
				except ValidationError:
					pass

	# Try to extract from model actions (look for done actions with data)
	for action_dict in reversed(history.model_actions()):
		if 'done' in action_dict:
			done_data = action_dict.get('done', {})
			if isinstance(done_data, dict) and 'data' in done_data:
				data = done_data['data']
				# Convert AnyHttpUrl to string if needed (deep copy to avoid modifying original)
				data_copy = copy.deepcopy(data) if isinstance(data, dict) else data
				if isinstance(data_copy, dict) and 'source_url' in data_copy:
					source_url = data_copy['source_url']
					# Handle AnyHttpUrl or other URL types
					if hasattr(source_url, '__str__') and not isinstance(source_url, str):
						data_copy['source_url'] = str(source_url)
					# Also handle startups list
					if 'startups' in data_copy and isinstance(data_copy['startups'], list):
						for startup in data_copy['startups']:
							if isinstance(startup, dict):
								# Ensure all URLs are strings
								for url_field in ['listing_url', 'linkedin_url']:
									if url_field in startup and startup[url_field] is not None:
										if hasattr(startup[url_field], '__str__') and not isinstance(startup[url_field], str):
											startup[url_field] = str(startup[url_field])
				try:
					report = StartupListingReport.model_validate(data_copy)
					if not agent_successful:
						print("âš ï¸  DonnÃ©es rÃ©cupÃ©rÃ©es depuis l'action 'done' malgrÃ© l'Ã©chec de l'agent.")
					return _sanitize_report(report)
				except ValidationError as e:
					# Try with JSON serialization first
					try:
						json_str = json.dumps(data_copy, default=str)
						report = StartupListingReport.model_validate_json(json_str)
						if not agent_successful:
							print("âš ï¸  DonnÃ©es rÃ©cupÃ©rÃ©es depuis l'action 'done' (aprÃ¨s conversion JSON) malgrÃ© l'Ã©chec de l'agent.")
						return _sanitize_report(report)
					except (ValidationError, json.JSONDecodeError):
						pass

	# If we get here, we couldn't extract any data
	if not agent_successful:
		print("âŒ Impossible d'extraire les donnÃ©es malgrÃ© plusieurs tentatives.")
	return _fallback_report(str(task_input.url), "L'agent a Ã©tÃ© interrompu avant de finaliser le JSON.")


def parse_arguments() -> StartupListingInput:
	"""Validate CLI arguments via Pydantic before launching the agent."""

	parser = argparse.ArgumentParser(description='Construit un listing de startups depuis une page Product Hunt/BetaList/etc.')
	parser.add_argument('url', help='URL du listing (Product Hunt, BetaList, FutureTools, etc.)')
	parser.add_argument(
		'--max-startups',
		type=int,
		default=12,
		help='Nombre maximal de startups Ã  extraire (par dÃ©faut: 12)',
	)
	parser.add_argument(
		'--output',
		default='startup_listings.json',
		help='Chemin du fichier JSON rÃ©sultat (par dÃ©faut: ./startup_listings.json)',
	)
	args = parser.parse_args()
	return StartupListingInput(url=args.url, max_startups=args.max_startups, output_path=Path(args.output))


async def main() -> None:
	"""CLI entry point."""

	try:
		task_input = parse_arguments()
		print(f"ğŸš€ DÃ©marrage de l'agent pour: {task_input.url}")
		print(f"ğŸ“Š Nombre max de startups: {task_input.max_startups}")
		print(f"ğŸ’¾ Fichier de sortie: {task_input.output_path}")
		
		result = await run_startup_listing(task_input)

		if result is None:
			print("âŒ L'agent n'a retournÃ© aucune donnÃ©e structurÃ©e.")
			return

		output_json = result.model_dump_json(indent=2, ensure_ascii=False)
		output_path = task_input.output_path
		output_path.parent.mkdir(parents=True, exist_ok=True)
		output_path.write_text(output_json, encoding='utf-8')

		print(output_json)
		print(f'\nâœ… Listing sauvegardÃ© dans: {output_path.resolve()}')
	except KeyboardInterrupt:
		print("\nâš ï¸  Interruption utilisateur dÃ©tectÃ©e.")
		raise
	except Exception as e:
		print(f"âŒ Erreur lors de l'exÃ©cution: {e}")
		import traceback
		traceback.print_exc()
		raise


if __name__ == '__main__':
	asyncio.run(main())
