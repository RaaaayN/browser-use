"""
Script de test pour v√©rifier la connexion LLM avec LiteLLM.

Ce script teste simplement si la connexion √† votre serveur LiteLLM fonctionne
sans lancer tout l'agent browser-use.
"""

from browser_use import ChatOpenAI
from dotenv import load_dotenv
import asyncio
import httpx
import time
import os
import sys

# Charge les variables d'environnement
load_dotenv()


async def test_network_connectivity():
	"""Teste d'abord la connectivit√© r√©seau de base."""
	url = os.getenv('OPENAI_API_URL', '')
	if not url:
		print("‚ùå OPENAI_API_URL non d√©fini dans .env")
		return False
	
	print("üåê Test de connectivit√© r√©seau...")
	print(f"   URL: {url}")
	
	try:
		# Test simple de connectivit√© avec un timeout court
		async with httpx.AsyncClient(timeout=10.0) as client:
			# Test avec un endpoint simple (health check si disponible)
			test_url = url.rstrip('/') + '/health' if not url.endswith('/v1') else url.rstrip('/v1') + '/health'
			try:
				response = await client.get(test_url)
				print(f"   ‚úÖ Serveur accessible (status: {response.status_code})")
				return True
			except httpx.TimeoutException:
				print("   ‚ö†Ô∏è  Timeout sur /health, mais le serveur peut √™tre accessible")
				return True
			except Exception as e:
				# Si /health n'existe pas, ce n'est pas grave
				print(f"   ‚ÑπÔ∏è  /health non disponible ({type(e).__name__}), testons directement l'API")
				return True
	except Exception as e:
		print(f"   ‚ùå Erreur de connectivit√©: {type(e).__name__}: {str(e)}")
		return False


async def test_llm_connection():
	"""Teste la connexion LLM avec un appel simple."""
	print("\nüîç Test de connexion LLM...")
	print(f"   URL: {os.getenv('OPENAI_API_URL', 'Non d√©fini')}")
	model_name = "gemini-2.5-flash-lite-preview-09-2025"
	print(f"   Mod√®le: {model_name}")
	print()
	
	# Test de connectivit√© d'abord
	if not await test_network_connectivity():
		print("\n‚ùå Probl√®me de connectivit√© r√©seau. V√©rifiez:")
		print("   1. Que le serveur LiteLLM est accessible")
		print("   2. Que vous √™tes sur le bon r√©seau/VPN")
		print("   3. Que l'URL est correcte")
		return False
	
	# Cr√©er le client LLM avec timeout raisonnable
	# Note: Le mod√®le fonctionne (test√© avec curl), mais le sch√©ma JSON structur√© peut √™tre lent
	llm = ChatOpenAI(
		model=model_name,
		timeout=httpx.Timeout(180.0, connect=30.0),  # 3 minutes (sch√©ma JSON peut √™tre lent)
		# Options de compatibilit√© Gemini
		add_schema_to_system_prompt=True,
		remove_min_items_from_schema=True,
		remove_defaults_from_schema=True,
	)
	
	# Message de test simple
	test_message = "R√©ponds simplement 'OK' si tu re√ßois ce message."
	
	print("üì§ Envoi du message de test (timeout: 180s pour sch√©ma JSON)...")
	start_time = time.time()
	
	try:
		# Test simple avec un message
		from browser_use.llm.messages import UserMessage
		messages = [UserMessage(content=test_message)]
		
		# Utiliser asyncio.wait_for pour forcer un timeout
		response = await asyncio.wait_for(
			llm.ainvoke(messages),
			timeout=185.0  # L√©g√®rement plus que le timeout HTTP
		)
		elapsed_time = time.time() - start_time
		
		print(f"\n‚úÖ Connexion r√©ussie!")
		print(f"   Temps de r√©ponse: {elapsed_time:.2f} secondes")
		print(f"   R√©ponse: {response.completion}")
		print(f"   Tokens utilis√©s: {response.usage.total_tokens if response.usage else 'N/A'}")
		return True
		
	except asyncio.TimeoutError:
		elapsed_time = time.time() - start_time
		print(f"\n‚ùå Timeout apr√®s {elapsed_time:.2f} secondes")
		print("   Le serveur LiteLLM ne r√©pond pas dans les temps.")
		print("   V√©rifiez:")
		print("   1. Que le serveur est en cours d'ex√©cution")
		print("   2. Que le mod√®le est disponible sur le serveur")
		print("   3. Que vous avez les bonnes permissions")
		return False
		
	except Exception as e:
		elapsed_time = time.time() - start_time
		print(f"\n‚ùå Erreur de connexion apr√®s {elapsed_time:.2f} secondes")
		print(f"   Type d'erreur: {type(e).__name__}")
		print(f"   Message: {str(e)}")
		
		# Suggestions selon le type d'erreur
		if "timeout" in str(e).lower() or "timed out" in str(e).lower():
			print("\n   üí° Suggestions:")
			print("      - Le serveur LiteLLM est peut-√™tre surcharg√©")
			print("      - Essayez avec un mod√®le plus rapide")
			print("      - V√©rifiez la latence r√©seau vers le serveur")
		elif "401" in str(e) or "unauthorized" in str(e).lower():
			print("\n   üí° V√©rifiez votre OPENAI_API_KEY")
		elif "404" in str(e) or "not found" in str(e).lower():
			print("\n   üí° V√©rifiez que l'URL est correcte et que le mod√®le existe")
		
		return False


if __name__ == "__main__":
	try:
		success = asyncio.run(test_llm_connection())
		sys.exit(0 if success else 1)
	except KeyboardInterrupt:
		print("\n\n‚ö†Ô∏è  Test interrompu par l'utilisateur")
		sys.exit(130)

